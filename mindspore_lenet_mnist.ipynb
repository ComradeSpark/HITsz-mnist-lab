{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 概述\n",
    "\n",
    "下面通过一个实际样例，带领大家体验MindSpore基础的功能。\n",
    "\n",
    "本例子会实现一个简单的图片分类的功能，整体流程如下：\n",
    "\n",
    "1. 处理需要的数据集，这里使用了MNIST数据集。\n",
    "2. 定义一个网络，这里我们使用LeNet网络。\n",
    "3. 自定义回调函数收集模型的损失值和精度值。\n",
    "4. 定义损失函数和优化器。\n",
    "5. 加载数据集并进行训练，训练完成后，查看结果及保存模型文件。\n",
    "6. 加载保存的模型，进行推理。\n",
    "7. 验证模型，加载测试数据集和训练后的模型，验证结果精度。\n",
    "\n",
    "这是简单、基础的应用流程，其他高级、复杂的应用可以基于这个基本流程进行扩展。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 准备环节\n",
    "\n",
    "在动手进行实践之前，确保，你已经正确安装了MindSpore。如果没有，可以通过[MindSpore安装页面](https://www.mindspore.cn/install)将MindSpore安装在你的电脑当中。  \n",
    "\n",
    "### 下载数据集\n",
    "\n",
    "示例中用到的`MNIST`数据集是由10类$28*28$的灰度图片组成，训练数据集包含60000张图片，测试数据集包含10000张图片。\n",
    "\n",
    "> MNIST数据集下载页面：<http://yann.lecun.com/exdb/mnist/>。页面提供4个数据集下载链接，其中前2个文件是训练数据需要，后2个文件是测试结果需要。示例中从华为云存储上下载MNIST数据集副本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!mkdir -p ./datasets/MNIST_Data/train ./datasets/MNIST_Data/test\n",
    "!wget -NP ./datasets/MNIST_Data/train https://mindspore-website.obs.myhuaweicloud.com/notebook/datasets/mnist/train-labels-idx1-ubyte \n",
    "!wget -NP ./datasets/MNIST_Data/train https://mindspore-website.obs.myhuaweicloud.com/notebook/datasets/mnist/train-images-idx3-ubyte\n",
    "!wget -NP ./datasets/MNIST_Data/test https://mindspore-website.obs.myhuaweicloud.com/notebook/datasets/mnist/t10k-labels-idx1-ubyte\n",
    "!wget -NP ./datasets/MNIST_Data/test https://mindspore-website.obs.myhuaweicloud.com/notebook/datasets/mnist/t10k-images-idx3-ubyte"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下载完之后，使用tree命令查看目录结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tree ./datasets/MNIST_Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 导入Python库&模块\n",
    "\n",
    "在使用前，需要导入需要的Python库。目前使用到`os`库，为方便理解，其他需要的库，我们在具体使用到时再说明。\n",
    "详细的MindSpore的模块说明，可以在[MindSpore API页面](https://www.mindspore.cn/doc/api_python/zh-CN/r1.2/index.html)中搜索查询。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T08:53:40.753790Z",
     "start_time": "2021-02-03T08:53:40.750705Z"
    }
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 配置运行信息\n",
    "\n",
    "在正式编写代码前，需要了解MindSpore运行所需要的硬件、后端等基本信息。\n",
    "\n",
    "可以通过`context.set_context`来配置运行需要的信息，譬如运行模式、后端信息、硬件等信息。\n",
    "\n",
    "导入`context`模块，配置运行需要的信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T08:53:41.528022Z",
     "start_time": "2021-02-03T08:53:40.755378Z"
    }
   },
   "outputs": [],
   "source": [
    "# conda install mindspore=2.0.0a0 -c mindspore -c conda-forge\n",
    "from mindspore import context\n",
    "\n",
    "context.set_context(mode=context.GRAPH_MODE, device_target=\"CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在样例中我们配置样例运行使用图模式。根据实际情况配置硬件信息，譬如代码运行在Ascend AI处理器上，则`--device_target`选择`Ascend`，代码运行在CPU、GPU同理。详细参数说明，请参见`context.set_context`接口说明。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据处理\n",
    "数据集对于训练非常重要，好的数据集可以有效提高训练精度和效率，在加载数据集前，通常会对数据集进行一些处理。\n",
    "\n",
    "由于后面会采用LeNet这样的卷积神经网络对数据集进行训练，而采用在训练数据时，对数据格式是有所要求的，所以接下来需要先查看数据集内的数据是什么样的，这样才能构造一个针对性的数据转换函数，将数据集数据转换成符合训练要求的数据形式。\n",
    "\n",
    "执行如下代码查看原始数据集数据。\n",
    "\n",
    "对于经典的MNIST等数据集，各大深度学习框架都集成了下载、预处理等操作，开发过程中只需要按照文档调用对应的api即可。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T08:53:42.128603Z",
     "start_time": "2021-02-03T08:53:41.530089Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib    # conda install -c conda-forge matplotlib\n",
    "import numpy as np\n",
    "import mindspore.dataset as ds\n",
    "\n",
    "train_data_path = \"./datasets/MNIST_Data/train\"\n",
    "test_data_path = \"./datasets/MNIST_Data/test\"\n",
    "mnist_ds = ds.MnistDataset(train_data_path)\n",
    "print('The type of mnist_ds:', type(mnist_ds))\n",
    "print(\"Number of pictures contained in the mnist_ds：\", mnist_ds.get_dataset_size())\n",
    "\n",
    "dic_ds = mnist_ds.create_dict_iterator()\n",
    "item = next(dic_ds)  # 取出一个样本\n",
    "img = item[\"image\"].asnumpy()\n",
    "label = item[\"label\"].asnumpy()\n",
    "\n",
    "print(\"The item of mnist_ds:\", item.keys())\n",
    "print(\"Tensor of image in item:\", img.shape) \n",
    "print(\"The label of item:\", label)\n",
    "\n",
    "img = np.squeeze(img)  # 从矩阵shape中，去掉维度为1的\n",
    "np.set_printoptions(linewidth=400)\n",
    "print(img)\n",
    "\n",
    "plt.imshow(img,cmap=\"gray\")  # 默认是彩色输出\n",
    "plt.title(\"number:%s\"% item[\"label\"].asnumpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从上面的运行情况我们可以看到，训练数据集`train-images-idx3-ubyte`和`train-labels-idx1-ubyte`对应的是6万张图片和6万个数字标签，载入数据后经过`create_dict_iterator`转换字典型的数据集，取其中的一个数据查看，这是一个key为`image`和`label`的字典，其中的`image`的张量(高度28，宽度28，通道1)和`label`为对应图片的数字。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义数据集及数据操作\n",
    "\n",
    "我们定义一个函数`create_dataset`来创建数据集。在这个函数中，我们定义好需要进行的数据增强和处理操作：\n",
    "\n",
    "1. 定义数据集。\n",
    "2. 定义进行数据增强和处理所需要的一些参数。\n",
    "3. 根据参数，生成对应的数据增强操作。\n",
    "4. 使用`map`映射函数，将数据操作应用到数据集。\n",
    "5. 对生成的数据集进行处理。\n",
    "\n",
    "定义完成后，使用`create_datasets`对原始数据进行增强操作，并抽取一个`batch`的数据，查看数据增强后的变化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T08:53:42.147143Z",
     "start_time": "2021-02-03T08:53:42.129614Z"
    }
   },
   "outputs": [],
   "source": [
    "import mindspore.dataset.vision as CV\n",
    "import mindspore.dataset.transforms as C\n",
    "# import mindspore.dataset.vision.c_transforms as CV\n",
    "# import mindspore.dataset.transforms.c_transforms as C\n",
    "from mindspore.dataset.vision import Inter\n",
    "from mindspore import dtype as mstype\n",
    "\n",
    "#预处理操作都注释掉，可根据需要打开\n",
    "def create_dataset(data_path, batch_size=32, repeat_size=1,\n",
    "                   num_parallel_workers=1):\n",
    "    \"\"\" \n",
    "    create dataset for train or test\n",
    "    \n",
    "    Args:\n",
    "        data_path (str): Data path\n",
    "        batch_size (int): The number of data records in each group\n",
    "        repeat_size (int): The number of replicated data records\n",
    "        num_parallel_workers (int): The number of parallel workers\n",
    "    \"\"\"\n",
    "    # define dataset\n",
    "    mnist_ds = ds.MnistDataset(data_path)\n",
    "\n",
    "    # define some parameters needed for data enhancement and rough justification\n",
    "    resize_height, resize_width = 32, 32\n",
    "    rescale = 1.0 / 255.0\n",
    "    shift = 0.0\n",
    "    rescale_nml = 1 / 0.3081\n",
    "    shift_nml = -1 * 0.1307 / 0.3081\n",
    "\n",
    "    # according to the parameters, generate the corresponding data enhancement method\n",
    "    resize_op = CV.Resize((resize_height, resize_width), interpolation=Inter.LINEAR)\n",
    "    rescale_nml_op = CV.Rescale(rescale_nml, shift_nml)\n",
    "    rescale_op = CV.Rescale(rescale, shift)\n",
    "    hwc2chw_op = CV.HWC2CHW()\n",
    "    type_cast_op = C.TypeCast(mstype.int32)\n",
    "\n",
    "    # using map to apply operations to a dataset\n",
    "    mnist_ds = mnist_ds.map(operations=type_cast_op, input_columns=\"label\", num_parallel_workers=num_parallel_workers)\n",
    "#     mnist_ds = mnist_ds.map(operations=resize_op, input_columns=\"image\", num_parallel_workers=num_parallel_workers)\n",
    "#     mnist_ds = mnist_ds.map(operations=rescale_op, input_columns=\"image\", num_parallel_workers=num_parallel_workers)\n",
    "#     mnist_ds = mnist_ds.map(operations=rescale_nml_op, input_columns=\"image\", num_parallel_workers=num_parallel_workers)\n",
    "    mnist_ds = mnist_ds.map(operations=hwc2chw_op, input_columns=\"image\", num_parallel_workers=num_parallel_workers)\n",
    "    mnist_ds = mnist_ds.map(operations=C.TypeCast(mstype.float32), input_columns=\"image\", num_parallel_workers=num_parallel_workers)\n",
    "    \n",
    "    # process the generated dataset\n",
    "    buffer_size = 10000\n",
    "    mnist_ds = mnist_ds.shuffle(buffer_size=buffer_size)\n",
    "    mnist_ds = mnist_ds.batch(batch_size, drop_remainder=True)\n",
    "    mnist_ds = mnist_ds.repeat(repeat_size)\n",
    "\n",
    "    return mnist_ds\n",
    "\n",
    "ms_dataset = create_dataset(train_data_path)\n",
    "print('Number of groups in the dataset:', ms_dataset.get_dataset_size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "调用数据增强函数后，查看数据集`size`由60000变成了1875，符合我们的数据增强中`mnist_ds.batch`操作的预期（$60000/32=1875$）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上述增强过程中：\n",
    "\n",
    "- 数据集中的`label`数据增强操作：\n",
    "\n",
    "  - `C.TypeCast`：将数据类型转化为`int32`。\n",
    "\n",
    "- 数据集中的`image`数据增强操作：  \n",
    "\n",
    "  - `datasets.MnistDataset`：将数据集转化为MindSpore可训练的数据。  \n",
    "  - `CV.Resize`：对图像数据像素进行缩放，适应LeNet网络对数据的尺寸要求。  \n",
    "  - `CV.Rescale`：对图像数据进行标准化、归一化操作，使得每个像素的数值大小在（0,1）范围中，可以提升训练效率。  \n",
    "  - `CV.HWC2CHW`：对图像数据张量进行变换，张量形式由`高x宽x通道`（HWC）变为`通道x高x宽`（CHW），方便进行数据训练。\n",
    " \n",
    "- 其他增强操作：\n",
    "\n",
    "  - `mnist_ds.shuffle`：随机将数据存放在可容纳10000张图片地址的内存中进行混洗。  \n",
    "  - `mnist_ds.batch`：从混洗的10000张图片地址中抽取32张图片组成一个`batch`，参数`batch_size`表示每组包含的数据个数，现设置每组包含32个数据。  \n",
    "  - `mnist_ds.repeat`：将`batch`数据进行复制增强，参数`repeat_size`表示数据集复制的数量。\n",
    "\n",
    "先进行`shuffle`、`batch`操作，再进行`repeat`操作，这样能保证1个`epoch`内数据不重复。\n",
    "\n",
    "> MindSpore支持进行多种数据处理和增强的操作，各种操作往往组合使用，具体可以参考[数据处理](https://www.mindspore.cn/doc/programming_guide/zh-CN/r1.2/pipeline.html)和与[数据增强](https://www.mindspore.cn/doc/programming_guide/zh-CN/r1.2/augmentation.html)章节。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 查看增强后的数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从1875组数据中取出一组数据，查看其数据张量及`label`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T08:53:42.580776Z",
     "start_time": "2021-02-03T08:53:42.149224Z"
    }
   },
   "outputs": [],
   "source": [
    "data = next(ms_dataset.create_dict_iterator(output_numpy=True))\n",
    "images = data[\"image\"]\n",
    "labels = data[\"label\"]\n",
    "print('Tensor of image:', images.shape)\n",
    "print('Labels:', labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将张量数据和`label`对应的值进行可视化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T08:53:43.245788Z",
     "start_time": "2021-02-03T08:53:42.582357Z"
    }
   },
   "outputs": [],
   "source": [
    "count = 1\n",
    "for i in images:\n",
    "    plt.subplot(4, 8, count) \n",
    "    plt.imshow(np.squeeze(i), cmap=\"gray\")\n",
    "    plt.title('num:%s'%labels[count-1])\n",
    "    plt.xticks([])\n",
    "    count += 1\n",
    "    plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过上述查询操作，看到经过变换后的图片，数据集内分成了1875组数据，每组数据中含有32张图片，每张图片像数值为32×32，数据全部准备好后，就可以进行下一步的数据训练了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们选择相对简单的LeNet网络。LeNet网络不包括输入层的情况下，共有7层：2个卷积层、2个下采样层（池化层）、3个全连接层。每层都包含不同数量的训练参数，如下图所示："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 更多的LeNet网络的介绍不在此赘述，希望详细了解LeNet网络，可以查询<http://yann.lecun.com/exdb/lenet/>。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在构建LeNet前，我们对全连接层以及卷积层采用Normal进行参数初始化。\n",
    "\n",
    "MindSpore支持`TruncatedNormal`、`Normal`、`Uniform`等多种参数初始化方法，具体可以参考MindSpore API的`mindspore.common.initializer`模块说明。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用MindSpore定义神经网络需要继承`mindspore.nn.Cell`，`Cell`是所有神经网络（`Conv2d`等）的基类。\n",
    "\n",
    "神经网络的各层需要预先在`__init__`方法中定义，然后通过定义`construct`方法来完成神经网络的前向构造，按照LeNet的网络结构，定义网络各层如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T08:53:43.367791Z",
     "start_time": "2021-02-03T08:53:43.248322Z"
    }
   },
   "outputs": [],
   "source": [
    "import mindspore.nn as nn\n",
    "from mindspore.common.initializer import Normal\n",
    "\n",
    "\n",
    "class LeNet5(nn.Cell):\n",
    "    \"\"\"Lenet network structure.\"\"\"\n",
    "    # define the operator required\n",
    "    def __init__(self, num_class=10, num_channel=1):\n",
    "        super(LeNet5, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(num_channel, 6, 5, pad_mode='pad', padding=2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5, pad_mode='valid')\n",
    "        self.fc1 = nn.Dense(16 * 5 * 5, 120, weight_init=Normal(0.02))\n",
    "        self.fc2 = nn.Dense(120, 84, weight_init=Normal(0.02))\n",
    "        self.fc3 = nn.Dense(84, num_class, weight_init=Normal(0.02))\n",
    "        self.relu = nn.ReLU() \n",
    "        self.avg_pool2d = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        self.flatten = nn.Flatten()\n",
    "        # self.dropout = nn.Dropout(0.5)   # dropout层添加方式\n",
    "\n",
    "    # use the preceding operators to construct networks\n",
    "    def construct(self, x):\n",
    "        x = self.avg_pool2d(self.relu(self.conv1(x)))\n",
    "        x = self.avg_pool2d(self.relu(self.conv2(x)))\n",
    "        x = self.flatten(x)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        #x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "network = LeNet5()\n",
    "print(\"layer conv1:\", network.conv1)\n",
    "print(\"*\"*40)\n",
    "print(\"layer fc1:\", network.fc1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "构建完成后，可以使用`print(LeNet5())`将神经网络中的各层参数全部打印出来，也可以使用`LeNet().{layer名称}`打印相应的参数信息。本例选择打印第一个卷积层和第一个全连接层的相应参数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 自定义回调函数收集模型的损失值和准确率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "自定义一个数据收集的回调类`StepLossAccInfo`，用于收集两类信息：\n",
    "\n",
    "1. 训练过程中`step`和`loss`值之间关系的信息；\n",
    "2. 每训练125个`step`和对应模型精度值`accuracy`的信息。\n",
    "\n",
    "该类继承了`Callback`类，可以自定义训练过程中的操作，等训练完成后，可将数据绘成图查看`step`与`loss`的变化情况，以及`step`与`accuracy`的变化情况。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下代码会作为回调函数，在模型训练函数`model.train`中调用，本文验证模型阶段会将收集到的信息，进行可视化展示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T08:53:43.400967Z",
     "start_time": "2021-02-03T08:53:43.369836Z"
    }
   },
   "outputs": [],
   "source": [
    "from mindspore.train.callback import Callback\n",
    "\n",
    "# custom callback function\n",
    "class StepLossAccInfo(Callback):\n",
    "    def __init__(self, model, eval_dataset, steps_loss, steps_eval):\n",
    "        self.model = model\n",
    "        self.eval_dataset = eval_dataset\n",
    "        self.steps_loss = steps_loss\n",
    "        self.steps_eval = steps_eval\n",
    "        \n",
    "    def step_end(self, run_context):\n",
    "        cb_params = run_context.original_args()\n",
    "        cur_epoch = cb_params.cur_epoch_num\n",
    "        cur_step = (cur_epoch-1)*1875 + cb_params.cur_step_num\n",
    "        self.steps_loss[\"loss_value\"].append(str(cb_params.net_outputs))\n",
    "        self.steps_loss[\"step\"].append(str(cur_step))\n",
    "        if cur_step % 125 == 0:\n",
    "            acc = self.model.eval(self.eval_dataset, dataset_sink_mode=False)\n",
    "            self.steps_eval[\"step\"].append(cur_step)\n",
    "            self.steps_eval[\"acc\"].append(acc[\"Accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中：\n",
    "\n",
    "- `model`：计算图模型Model。\n",
    "- `eval_dataset`：验证数据集。\n",
    "- `steps_loss`：收集step和loss值之间的关系，数据格式`{\"step\": [], \"loss_value\": []}`。\n",
    "- `steps_eval`：收集step对应模型准确率`accuracy`的信息，数据格式为`{\"step\": [], \"acc\": []}`。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义损失函数及优化器\n",
    "\n",
    "在进行定义之前，先简单介绍损失函数及优化器的概念。\n",
    "\n",
    "- 损失函数：又叫目标函数，用于衡量预测值与实际值差异的程度。深度学习通过不停地迭代来缩小损失函数的值。定义一个好的损失函数，可以有效提高模型的性能。\n",
    "\n",
    "- 优化器：用于最小化损失函数，从而在训练过程中改进模型。\n",
    "\n",
    "定义了损失函数后，可以得到损失函数关于权重的梯度。梯度用于指示优化器优化权重的方向，以提高模型性能。\n",
    "\n",
    "MindSpore支持的损失函数有`SoftmaxCrossEntropyWithLogits`、`L1Loss`、`MSELoss`等。这里使用`SoftmaxCrossEntropyWithLogits`损失函数。\n",
    "\n",
    "MindSpore支持的优化器有`SGD`、`Adam`、`AdamWeightDecay`、`Momentum`等。这里使用`SGD`优化器。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T08:53:43.478821Z",
     "start_time": "2021-02-03T08:53:43.402997Z"
    }
   },
   "outputs": [],
   "source": [
    "import mindspore.nn as nn\n",
    "from mindspore.nn import SoftmaxCrossEntropyWithLogits\n",
    "\n",
    "# create the network\n",
    "network = LeNet5()\n",
    "\n",
    "# define the optimizer\n",
    "net_opt = nn.SGD(network.trainable_params(), learning_rate = 0.01) \n",
    "# net_opt = nn.Momentum(network.trainable_params(), 0.01, 0.9)   # \n",
    "\n",
    "# define the loss function\n",
    "# net_loss = SoftmaxCrossEntropyWithLogits(sparse=True, reduction='mean')\n",
    "net_loss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "完成神经网络的构建后，就可以着手进行网络训练了，通过MindSpore提供的`Model.train`接口可以方便地进行网络的训练，参数主要包含：\n",
    "\n",
    "1. 每个`epoch`需要遍历完成图片的batch数：`epoch_size`；  \n",
    "2. 训练数据集`ds_train`；  \n",
    "3. MindSpore提供了callback机制，回调函数`callbacks`，包含`ModelCheckpoint`、`LossMonitor`和`Callback`模型检测参数；其中`ModelCheckpoint`可以保存网络模型和参数，以便进行后续的fine-tuning（微调）操作；    \n",
    "4. 数据下沉模式`dataset_sink_mode`，此参数默认`True`需设置成`False`，因为此模式不支持CPU计算平台。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T08:54:23.928574Z",
     "start_time": "2021-02-03T08:53:43.479829Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from mindspore import Tensor, Model\n",
    "from mindspore.train.callback import ModelCheckpoint, CheckpointConfig, LossMonitor\n",
    "from mindspore.nn import Accuracy\n",
    "\n",
    "epoch_size = 1\n",
    "mnist_path = \"./datasets/MNIST_Data\"\n",
    "model_path = \"./models/ckpt/mindspore_quick_start/\"\n",
    "\n",
    "repeat_size = 1\n",
    "ds_train = create_dataset(os.path.join(mnist_path, \"train\"), 32, repeat_size)\n",
    "eval_dataset = create_dataset(os.path.join(mnist_path, \"test\"), 32)\n",
    "\n",
    "# clean up old run files before in Linux\n",
    "os.system('rm -f {0}*.ckpt {0}*.meta {0}*.pb'.format(model_path))\n",
    "\n",
    "# define the model\n",
    "model = Model(network, net_loss, net_opt, metrics={\"Accuracy\": Accuracy()}) \n",
    "\n",
    "# save the network model and parameters for subsequence fine-tuning\n",
    "config_ck = CheckpointConfig(save_checkpoint_steps=375, keep_checkpoint_max=16)\n",
    "# group layers into an object with training and evaluation features\n",
    "ckpoint_cb = ModelCheckpoint(prefix=\"checkpoint_lenet\", directory=model_path, config=config_ck) \n",
    "\n",
    "steps_loss = {\"step\": [], \"loss_value\": []}\n",
    "steps_eval = {\"step\": [], \"acc\": []}\n",
    "# collect the steps,loss and accuracy information\n",
    "step_loss_acc_info = StepLossAccInfo(model , eval_dataset, steps_loss, steps_eval)\n",
    "\n",
    "\n",
    "model.train(epoch_size, ds_train, callbacks=[ckpoint_cb, LossMonitor(125), step_loss_acc_info], dataset_sink_mode=False)\n",
    "\n",
    "print(\"train finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "运行上述代码输出loss是训练集的loss。训练完成后，会在设置的模型保存路径上生成多个模型文件。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T08:54:23.979139Z",
     "start_time": "2021-02-03T08:54:23.929589Z"
    }
   },
   "outputs": [],
   "source": [
    "!tree $model_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-01T03:42:45.627283Z",
     "start_time": "2021-02-01T03:42:45.606466Z"
    }
   },
   "source": [
    "文件名称具体含义`{ModelCheckpoint中设置的自定义名称}-{第几个epoch}_{第几个step}.ckpt`。\n",
    "\n",
    "> 使用自由控制循环的迭代次数、遍历数据集等，可以参照官网编程指南《[训练](https://www.mindspore.cn/doc/programming_guide/zh-CN/r1.2/train.html#%E8%87%AA%E5%AE%9A%E4%B9%89%E8%AE%AD%E7%BB%83%E5%BE%AA%E7%8E%AF)》的自定义循环训练部分。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 查看模型loss和accuracy随着训练步数在测试集上的变化情况"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T08:54:25.213489Z",
     "start_time": "2021-02-03T08:54:25.078949Z"
    }
   },
   "outputs": [],
   "source": [
    "def eval_show(steps_data, metric):\n",
    "    plt.xlabel(\"step number\")\n",
    "    plt.ylabel(\"Model \" + metric)\n",
    "    plt.title(\"Model \" + metric + \" variation chart\")\n",
    "    plt.plot(steps_data[\"step\"], steps_data[metric], \"red\")\n",
    "    plt.show()\n",
    "\n",
    "steps = steps_loss[\"step\"]\n",
    "loss_value = steps_loss[\"loss_value\"]\n",
    "steps_loss[\"step\"] = list(map(int, steps))\n",
    "steps_loss[\"loss_value\"] = list(map(float, loss_value))\n",
    "\n",
    "eval_show(steps_loss, 'loss_value')\n",
    "eval_show(steps_eval, 'acc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从上面可以看出来loss变化大致分为三个阶段：\n",
    "\n",
    "阶段一：训练开始时，loss值在2.2上下浮动。\n",
    "\n",
    "阶段二：训练到某一时刻，loss值迅速减少，训练收益大幅增加。\n",
    "\n",
    "阶段三：loss值收敛到一定小的值后，开始振荡在一个小的区间上无法趋0，再继续增加训练并无明显收益，至此训练结束。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loss每个batch都会记录一个值，准确率每125个batch才记录一次，其中`steps_eval`存储着模型的step数和对应模型准确率信息。\n",
    "\n",
    "从图中可以看出训练得到的模型准确率变化分为三个阶段，跟loss的变化是对应的，但变化趋势相反。\n",
    "\n",
    "阶段一：训练开始时，模型精度缓慢震荡上升。\n",
    "\n",
    "阶段二：训练到某一时刻，模型精度迅速上升。\n",
    "\n",
    "阶段三：缓慢上升趋近于不到1的某个值时附近振荡。\n",
    "\n",
    "整个训练过程，随着训练数据的增加，会对模型精度有着正相关的影响，但是随着精度到达一定程度，训练收益会降低。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  验证模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一般最后保存的模型效果最好，通过在测试数据集评估准确度，验证模型的泛化能力。\n",
    "\n",
    "验证模型的过程主要为：\n",
    "\n",
    "1. 载入模型`.ckpt`文件中的参数`param_dict`；\n",
    "2. 将参数`param_dict`载入到神经网络LeNet中；\n",
    "3. 载入测试数据集；\n",
    "4. 调用函数`model.eval`传入参数测试数据集`ds_eval`，生成模型`checkpoint_lenet-{epoch}_1875.ckpt`的精度值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T08:54:25.073201Z",
     "start_time": "2021-02-03T08:54:24.124960Z"
    }
   },
   "outputs": [],
   "source": [
    "from mindspore import load_checkpoint, load_param_into_net\n",
    "\n",
    "# testing relate modules \n",
    "def test_net(network, model, mnist_path):\n",
    "    \"\"\"Define the evaluation method.\"\"\"\n",
    "    print(\"============== Starting Testing ==============\")\n",
    "    # load the saved model for evaluation\n",
    "    param_dict = load_checkpoint(\"./models/ckpt/mindspore_quick_start/checkpoint_lenet-1_1875.ckpt\")\n",
    "    # load parameter to the network\n",
    "    load_param_into_net(network, param_dict)\n",
    "    \n",
    "    # load testing dataset\n",
    "    ds_eval = create_dataset(os.path.join(mnist_path, \"test\"))\n",
    "    \n",
    "    # 此处的model是训练之后的，并非是加载param_dict得到的模型\n",
    "    acc = model.eval(ds_eval, dataset_sink_mode=False)\n",
    "    print(\"============== Test data Accuracy:{} ==============\".format(acc))\n",
    "\n",
    "test_net(network, model, mnist_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中：\n",
    "\n",
    "- `load_checkpoint`：通过该接口加载CheckPoint模型参数文件，返回一个参数字典。\n",
    "\n",
    "- `checkpoint_lenet-1_1875.ckpt`：之前保存的CheckPoint模型文件名称。\n",
    "\n",
    "- `load_param_into_net`：通过该接口把参数加载到网络中。\n",
    "\n",
    "经过1875步训练后生成的模型精度超过95%，模型优良。\n",
    "\n",
    "我们可以看一下模型随着训练步数变化，精度随之变化的情况。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 推理预测"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们使用生成的模型应用到分类预测单个或者单组图片数据上，具体步骤如下："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 将要测试的数据转换成适应LeNet的数据类型。\n",
    "2. 提取出`image`的数据。\n",
    "3. 使用函数`model.predict`预测`image`对应的数字。`predict`返回的是`image`对应0-9的各个可能的值，最大的值是最有可能的分类。\n",
    "4. 调用`plot`将预测出的各数字显示出来。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "载入要预测的数据集，并调用`create_dataset`转换成符合格式要求的数据集，并选取其中一组32张图片进行推理预测。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T08:54:26.317823Z",
     "start_time": "2021-02-03T08:54:25.214530Z"
    }
   },
   "outputs": [],
   "source": [
    "ds_test = create_dataset(test_data_path).create_dict_iterator()\n",
    "data = next(ds_test)\n",
    "images = data[\"image\"].asnumpy()\n",
    "labels = data[\"label\"].asnumpy()\n",
    "\n",
    "output = model.predict(Tensor(data['image']))\n",
    "unnorm_result = output[0]  # output[0]模型预测第一个测试图片的结果输出\n",
    "print(unnorm_result)  \n",
    "#@TODO，要求unnorm_result中的数据归一化\n",
    "\n",
    "pred = np.argmax(output.asnumpy(), axis=1)\n",
    "err_num = []\n",
    "index = 1\n",
    "for i in range(len(labels)):\n",
    "    plt.subplot(4, 8, i+1)\n",
    "    color = 'blue' if pred[i] == labels[i] else 'red'\n",
    "    plt.title(\"pre:{}\".format(pred[i]), color=color)\n",
    "    plt.imshow(np.squeeze(images[i]),cmap=\"gray\")\n",
    "    plt.axis(\"off\")\n",
    "    if color == 'red':\n",
    "        index = 0\n",
    "        print(\"Row {}, column {} is incorrectly identified as {}, the correct value should be {}\".format(int(i/8)+1, i%8+1, pred[i], labels[i]), '\\n')\n",
    "if index:\n",
    "    print(\"All the figures in this group are predicted correctly!\")\n",
    "print(pred, \"<--Predicted figures\") \n",
    "print(labels, \"<--The right number\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "主要参考：https://www.mindspore.cn/tutorial/training/zh-CN/r1.2/quick_start/quick_start.html\n",
    "本示例用MNIST test数据集进行训练过程的模型验证，有些资料中会通过从训练集中划分一部分出来做验证集，\n",
    "区别不大，模型训练的主要过程是一致的。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
